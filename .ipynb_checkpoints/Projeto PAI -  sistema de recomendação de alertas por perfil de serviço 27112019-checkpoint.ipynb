{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando todas as bibliotecas para os possíveis testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Vanessa\n",
      "[nltk_data]     Eufrauzino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Vanessa\n",
      "[nltk_data]     Eufrauzino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo todos os arquivos de alerta de sms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2066"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserindo as stopwords a serem retiradas dos textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extraindo os textos das páginas dos pdf's e transformando em objetos python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05d020d0e114732984e08a2048af5b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2066), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-29 14:24:23,221 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "textos_alertas=[]\n",
    "nomes=[]\n",
    "erros=[]\n",
    "for i in tqdm_notebook(range(len(arquivos))):\n",
    "    try:\n",
    "        temp= parser.from_file('C:\\\\Users\\\\Vanessa Eufrauzino\\\\Documents\\\\alertas-portugues\\\\pdf\\\\'+ arquivos[i])['content']\n",
    "        temp = temp.lower()\n",
    "        nomes.append(arquivos[i])\n",
    "        textos_alertas.append( ' '.join(map(str, [word for word in word_tokenize(temp) if word not in stop_words])))\n",
    "    except Exception as e:\n",
    "        erros.append([e,i])\n",
    "        continue        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_alertas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(nomes)\n",
    "#nomes.append(\"Etapa\")\n",
    "#textos_alertas.append(\"manut preventiva mecanica guindaste conves gd-z-80005-a mod 13, com uso de ferramentas manuais. om: 2018378481/ 2018378480/ 2048378479/ 2018378474/ 2018378482/ 2018378408/ 20183784831 2018378484/ 20183784091201837848512018378486/ 2018378476\")\n",
    "#len(textos_alertas)\n",
    "df_alertas = pd.DataFrame({\"Nome arquivo\":nomes,\"conteudo\":textos_alertas})\n",
    "#df_alertas.set_index(\"Nome arquivo\")\n",
    "#df_alertas.loc(\"Alerta de SMS PB 001 2019 - ACIDENTE FATAL - ESMAGAMENTO DURANTE MANUTENCAO DE GUINDASTE.pdf\")\n",
    "#df_alertas.to_csv('df_alertas.csv', sep = ';', encoding='utf-8')\n",
    "#nomes[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraindo os textos de paginas\n",
    "textos=[]\n",
    "nome=[]\n",
    "for i in tqdm_notebook(range(len(arquivos2))):\n",
    "    temp = open('/home/ec2-user/SageMaker/AmazonSageMaker-pai-sagemaker-python-git/alertas/' + arquivos[i], 'rb')#insere no python\n",
    "    pdf = PyPDF2.PdfFileReader(temp)#cria objeto \n",
    "    for j in range (pdf.getNumPages()):\n",
    "        textos.append(pdf.getPage(j).extractText())#retira o texto\n",
    "        nome.append(arquivos2[i])\n",
    "    for k in range(len(textos)):\n",
    "        textos[k]=textos[k].lower()\n",
    "        textos[k]=textos[k].replace(\"\\n\",\"\")\n",
    "        textos[k]=textos[k].replace(\"   \",\" \")\n",
    "        textos[k] = ' '.join(map(str, [word for word in word_tokenize(textos[k]) if word not in stop_words]))\n",
    "#type(texto)#diz a classe do objeto --str\n",
    "#palavras=texto.split('\\n') #separa cada sentença pelo operador definido, nesse caso \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfetap = pd.read_excel('C:/Users/Vanessa Eufrauzino/Documents/conjunto de dados do PAI/Resultado_aplat_etapas_servicos_p_40_30_11_2019.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcde91165b314b55bd18cd89abc98be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for k in tqdm_notebook(range(len(dfetap['ETPL_TX_DESCRICAO']))):\n",
    "    textos = dfetap['ETPL_TX_DESCRICAO'][k]\n",
    "    textos = textos.lower()\n",
    "    textos = textos.replace(\"\\n\",\"\")\n",
    "    textos = textos.replace(\"   \",\" \")\n",
    "    textos = ' '.join(map(str, [word for word in word_tokenize(textos) if word not in stop_words]))\n",
    "    dfetap['ETPL_TX_DESCRICAO'][k]=textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abordagem 1\n",
    "\n",
    "Utilizando a lib spaCy para a observância dos dados, observando a mínima similaridade possível feita com uma biblioteca em português de noticias com quantidade baixa de tokens no corpus. Como essa abordagem demora um pouco, foi sendo incrementado o conjunto de dados. *Não consigo instalar no sagemaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nlp = spacy.load (\"pt_core_news_sm\")#(\"xx_ent_wiki_sm\")\n",
    "cont=0\n",
    "cont2=0\n",
    "with open('similaridadefile2.csv','w+') as similaridade2_file:\n",
    "    similaridade2_writer = csv.writer(similaridade2_file, delimiter=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for sms in tqdm_notebook(textos):\n",
    "        sms_i = nlp(sms)\n",
    "        for n in c:\n",
    "            similaridade2_writer.writerow([nome_arq[cont],cd_pt[cont2],cd_plat[cont2], nlp(n), sms_i.similarity(nlp(n))]) \n",
    "            cont2+=1\n",
    "        cont+=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abordagem 2\n",
    "\n",
    "A abordagem explícita de similaridade de cosseno, usando a mesma técnica do tfidf com uma rotina própria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t=[dfetap['texto_concat'][0:4]]\n",
    "#for i in range('texto_concat'][0:4]):\n",
    "#    t.append(dfetap['texto_concat'][i])\n",
    "    \n",
    "#teste=dfetap['texto_concat'].tolist()   \n",
    "len(textos_alertas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfetap.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1997"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fit_transform(text, words=vocabulario):\n",
    "    \"\"\"\n",
    "    Converte o texto em um vetor, onde compara se cada palavra obtida no vetor de \n",
    "    todas as palavras contém ou não em cada texto. \n",
    "    Insere 1 se sim e 0 se não.\n",
    "    \"\"\"\n",
    "    #return [1 if word in text.split() else 0 for word in words]\n",
    "    return [int(word in text.split()) for word in words]\n",
    "\n",
    "features2 = np.array(list(map(fit_transform, dfetap['ETPL_TX_DESCRICAO'])))\n",
    "\n",
    "len(features2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#etapa_guindaste# = \"MANUT PREVENTIVA MECÂNICA GUINDASTE CONVÉS GD-Z-80005-A MOD 13, COM USO DE FERRAMENTAS MANUAIS. OM: 2018378481/ 2018378480/ 2048378479/ 2018378474/ 2018378482/ 2018378408/ 20183784831 2018378484/ 20183784091201837848512018378486/ 2018378476\"\n",
    "#etapa_guindaste #= [etapa_guindaste.lower()]\n",
    "API_ENDPOINT='https://api-pai-dev.transformacaodigitalspassu.com.br/api/pai/registraalerta/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d28ced9dc7554a8c8fd4a998f24bef94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "similaridade = []\n",
    "for etapa in tqdm_notebook (range(len(dfetap['ETPL_TX_DESCRICAO']))):\n",
    "    for x, t, s in text_simillarities(etapa, dfetap['ETPL_TX_DESCRICAO'][etapa]):\n",
    "         similaridade.append([dfetap['ETPL_CD_ETAPA'][etapa], dfetap['PLAT_CD_PLATAF'][etapa], dfetap['SERV_NR_ANO'][etapa],\n",
    "                              dfetap['SERV_CD_SERVICO'][etapa],dfetap['ETPL_TX_DESCRICAO'][etapa], x, round(s, 8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = ['ETPL_CD_ETAPA','PLAT_CD_PLATAF','SERV_NR_ANO','SERV_CD_SERVICO','ETPL_TX_DESCRICAO','NOME_ARQUIVO','SIMILARIDADE']\n",
    "similar= pd.DataFrame(similaridade, columns = colunas)\n",
    "similar.to_csv(\"C:/Users/Vanessa Eufrauzino/Documents/conjunto de dados do PAI/similar29-11.csv\", encoding  = \"latin-1\", sep = \";\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Texto analisado -> ',dfetap['texto_concat'][3], '\\n')\n",
    "alerts= []\n",
    "for x, t, s in text_simillarities(28):\n",
    "    alerts.append([x, round(s, 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_alertas.set_index(\"Nome arquivo\")\n",
    "#df=pd.DataFrame(alerts)\n",
    "df.to_csv(\"./Nova pasta/alertas_guindaste.csv\", encoding = 'utf-8', sep = \";\")\n",
    "#df_alertas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abordagem 3, teste dos modelos de classificação com a planilha de dados do validador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validador=pd.read_excel('C:/Users/Vanessa Eufrauzino/Documents/alertas-portugues/petrobras/testes de corpus/df_validador_alertas.xlsx')\n",
    "#len(df_validador1['ALERTAS '])\n",
    "df_validador.columns                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parar=0\n",
    "while parar <543:\n",
    "    for i in tqdm_notebook(range(len(dftextos['nome_arquivo']))):\n",
    "        for j in tqdm_notebook(range(len(df_validador1['ALERTAS ']))):\n",
    "                if (dftextos['nome_arquivo'][i] == df_validador1['ALERTAS '][j]):\n",
    "                               df_validador1[\"Textos\"]= dftextos['alertas'][i]\n",
    "                               parar+=1                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testando mais uma similaridade de cosseno com bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "from unicodedata import normalize\n",
    "from nltk import ngrams\n",
    "\n",
    "#Regex para encontrar tokens\n",
    "REGEX_WORD = re.compile(r'\\w+')\n",
    "#Numero de tokens em sequencia\n",
    "N_GRAM_TOKEN = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faz a normalizacao do texto removendo espacos a mais e tirando acentos\n",
    "def text_normalizer(src):\n",
    "    return re.sub('\\s+', ' ',\n",
    "                normalize('NFKD', src)\n",
    "                   .encode('ASCII','ignore')\n",
    "                   .decode('ASCII')).lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faz o calculo de similaridade baseada no coseno\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        coef = float(numerator) / denominator\n",
    "        if coef > 1:\n",
    "            coef = 1\n",
    "        return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monta o vetor de frequencia da sentenca\n",
    "def sentence_to_vector(text, use_text_bigram):\n",
    "    words = REGEX_WORD.findall(text)\n",
    "    accumulator = []\n",
    "    for n in range(1,N_GRAM_TOKEN):\n",
    "        gramas = ngrams(words, n)\n",
    "        for grama in gramas:\n",
    "            accumulator.append(str(grama))\n",
    "\n",
    "    if use_text_bigram:\n",
    "        pairs = get_text_bigrams(text)\n",
    "        for pair in pairs:\n",
    "            accumulator.append(pair)\n",
    "\n",
    "    return Counter(accumulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtem a similaridade entre duas sentencas\n",
    "def get_sentence_similarity(sentence1, sentence2, use_text_bigram=False):\n",
    "    vector1 = sentence_to_vector(text_normalizer(sentence1), use_text_bigram)\n",
    "    vector2 = sentence_to_vector(text_normalizer(sentence2), use_text_bigram)\n",
    "    return cosine_similarity(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metodo de gerar bigramas de uma string\n",
    "def get_text_bigrams(src):\n",
    "    s = src.lower()\n",
    "    return [s[i:i+2] for i in range(len(s) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "similaridades=[]\n",
    "for i in tqdm_notebook(range(10000)):   \n",
    "    if __name__ == \"__main__\":\n",
    "        w1 = dfetap['texto_concat'][i]\n",
    "        words = textos \n",
    "\n",
    "        #print('Busca: ' + w1)\n",
    "\n",
    "        #Nivel de aceite (40%)\n",
    "        #cutoff = 0.40\n",
    "        #Sentenças similares\n",
    "        \n",
    "        for w2 in words:\n",
    "            #print('\\nCosine Sentence --- ' + w2)\n",
    "\n",
    "            #Calculo usando similaridade do coseno com apenas tokens\n",
    "            similarity_sentence = get_sentence_similarity(w1, w2)\n",
    "            #print('\\tSimilarity sentence: ' + str(similarity_sentence))\n",
    "\n",
    "            #Calculo usando similaridade do coseno com tokens e com ngramas do texto\n",
    "            #similarity_sentence_text_bigram = get_sentence_similarity(w1, w2, use_text_bigram=True)\n",
    "            #print('\\tSimilarity sentence: ' + str(similarity_sentence_text_bigram))\n",
    "            similaridades.append((w1,w2,str(similarity_sentence)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfsimilaridades = pd.DataFrame(similaridades, columns =['etapa_planejamento', 'Alertas', 'sim_cosseno','sim_cosseno_bigram']) \n",
    "#dfsimilaridades\n",
    "dfsimilaridades.to_csv(r'C:/Users/Vanessa Eufrauzino/Documents/alertas-portugues/petrobras/testes de corpus/similaridade2.csv',sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validador.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_validador['ETPL_TX_DESCRICAO'],df_validador['Textos'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clfRFC = RandomForestClassifier().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_validador['ETPL_TX_DESCRICAO'],df_validador['Textos'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clfM = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest=[]\n",
    "multi=[]\n",
    "paradas=[]\n",
    "for i in tqdm_notebook(range(len(dfetap['texto_concat']))):\n",
    "    try:\n",
    "        z=dfetap['texto_concat'][i]\n",
    "        multi.append([z,clfM.predict(count_vect.transform([z]))])\n",
    "        forest.append([z,clfRFC.predict(count_vect.transform([z]))])  \n",
    "    except Exception as e:\n",
    "        paradas.append((z,i))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffloresta = pd.DataFrame(forest, columns =['etapa_serviço', 'alertas']) \n",
    "dfmulti = pd.DataFrame(multi, columns =['etapa_serviço', 'alertas']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfmulti.to_csv(r'C:/Users/Vanessa Eufrauzino/Documents/alertas-portugues/petrobras/testes de corpus/multinomial.csv', sep=';')\n",
    "#dffloresta.to_csv(r'C:/Users/Vanessa Eufrauzino/Documents/alertas-portugues/petrobras/testes de corpus/floresta.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_new = pd.DataFrame({'Etapa_serviço_floresta':dffloresta['etapa_serviço'],'Alertas_floresta':dffloresta['alertas'],\n",
    "                  #     'Etapa_serviço_Multinomial':dfmulti['etapa_serviço'],'Alertas_Multinomial':dfmulti['alertas']}\n",
    "df_new.columns    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm_notebook(range(len(df_new))):\n",
    "    if df_new['Alertas_floresta'][i]==df_new['Alertas_Multinomial'][i]:\n",
    "        df_new['Comparação']=1\n",
    "    else:   \n",
    "        df_new['Comparação']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "[In] y = np.array([1, 1, 2, 2])\n",
    "[In] pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
    "[In] \n",
    "[In] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
