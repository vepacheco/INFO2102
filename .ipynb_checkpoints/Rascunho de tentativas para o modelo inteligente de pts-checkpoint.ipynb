{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Rascunho de tentativas e erros, para só levar os comandos finais para o script principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer().fit_transform(PNA1['ETPL_TX_DESCRICAO'])\n",
    "# no need to normalize, since Vectorizer will return normalized tf-idf\n",
    "pairwise_similarity = tfidf * tfidf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus =                                                                                                                                                                                                 \n",
    "vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n",
    "tfidf = vect.fit_transform(corpus)                                                                                                                                                                                                                       \n",
    "pairwise_similarity = tfidf * tfidf.T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp   \n",
    "X_train = tfid.fit_transform(textos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rascunhos de comando que comecei usar e não precisei\n",
    " #os.listdir(os.getcwd()) lista tudo o que tem no meu diretório\n",
    "#df=pd.read_csv(arquivos[1], encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####transforma tudo em csv funcionou bonitinho, mas nao vou precisar\n",
    "arquivos = [arq for arq in os.listdir(os.getcwd()) if arq.lower().endswith(\".csv\")]\n",
    "print(len(arquivos))\n",
    "#extraindo os textos de paginas\n",
    "\n",
    "for i in tqdm_notebook(range(len(arquivos))):\n",
    "    teste[i] = open(arquivos[i], 'rb')#insere no python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = tabula.read_pdf(\"2018_12_29_SRGE_P68_ALERTA_Queda passarela da gangway.pdf\")\n",
    "#tabula.convert_into(\"2018_12_29_SRGE_P68_ALERTA_Queda passarela da gangway.pdf\", \"output.csv\", output_format=\"csv\")\n",
    "#converte apenas um em csv\n",
    "#df=pd.read_csv(\"2018_12_29_SRGE_P68_ALERTA_Queda passarela da gangway.csv\",encoding=\"cp1250\")\n",
    "#df.head\n",
    "#obj=df['ALERTA DE SMS No: PB 001/2019'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insiro o conjunto de palavras desnecessarias em um conjunto\n",
    "#Criando um objeto que é um conjunto (set) para as palavras, \n",
    "#conectivos e outros com palavras em espanhol, inglês e português. \n",
    "#Para mesclar tanto ingles quanto o espanhol no conjunto com o portugues, utilizei o comando update.\n",
    "#z=set(stopwords.words('portuguese'))\n",
    "#z.update(set(stopwords.words('english')))\n",
    "#\n",
    "#type(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exemplos para a tentativa de criação do modelo, tendo algumas descritivas juntas\n",
    "df = pd.read_csv('Consumer_Complaints.csv')\n",
    "len(df)\n",
    "df = df[['Consumer complaint narrative','Product']]\n",
    "df = df[pd.notnull(df['Consumer complaint narrative'])]\n",
    "df.rename(columns = {'Consumer complaint narrative':'narrative'}, inplace = True)\n",
    "df.head(10)\n",
    "df.shape\n",
    "df.index = range (393912) \n",
    "df ['narrative']. apply (lambda x: len (x.split (' '))). sum ()\n",
    "cnt_pro = df['Product'].value_counts()\n",
    "# plt.figure(figsize=(12,4))\n",
    "# sns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\n",
    "# plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "# plt.xlabel('Product', fontsize=12)\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show();\n",
    "def print_complaint(index):\n",
    "    example = df[df.index == index][['narrative', 'Product']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print(example[0])\n",
    "        print('Product:', example[1])\n",
    "print_complaint(20)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "def cleanText(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\\n",
    "\\\n",
    "\\\n",
    "', r' ', text) \n",
    "    text = re.sub(r'http\\S+', r'<URL>', text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('x', '')\n",
    "    return text\n",
    "df['narrative'] = df['narrative'].apply(cleanText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "train_tagged = train.apply(lambda r: TaggedDocument(words=tokenize_text(r['narrative']), tags=[r.Product]), axis=1)\n",
    "test_tagged = test.apply(lambda r: TaggedDocument(words=tokenize_text(r['narrative']), tags=[r.Product]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Consumer_Complaints.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abrir o pdf no python e criar um objeto que possa abrir o texto e retornar as linhas que eu quero \n",
    "f = open('teste_pdf.pdf', 'rb8')#insere no python\n",
    "pdf = PyPDF2.PdfFileReader(f)#cria objeto \n",
    "pdf.getNumPages()#numero de páginas\n",
    "\n",
    "content = pdf.getPage(1).extractText()\n",
    "#type(pdf)\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#está funcionando vetorização de palavras\n",
    "#cvec = CountVectorizer(stop_words= z , min_df=2, max_df=0.5, ngram_range=(1,2))\n",
    "#sf = cvec.fit_transform(arquivos[1:5])\n",
    "#list(cvec.vocabulary_.keys())[:200]\n",
    "#transformer = TfidfTransformer()\n",
    "#transformed_weights = transformer.fit_transform(sf)\n",
    "#weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\n",
    "#weights_df = pd.DataFrame({'term': cvec.get_feature_names(), 'weight': weights})\n",
    "#weights_df.sort_values(by='weight', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_stopwords(sentence, idioma):\n",
    "    \"\"\"Função que recebe texto como entrada e devolve lista\n",
    "    de palavras excluindo pontuacao, preposicoes, artigos,\n",
    "    palavra com 1 letra, etc.\n",
    "    \"\"\"\n",
    "    if idioma == \"inglês\":\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        phrase = []\n",
    "        for word in sentence:\n",
    "            if word not in stopwords and word not in string.punctuation and len(word)>1:\n",
    "                phrase.append(word)\n",
    "        return phrase\n",
    "    elif idioma == \"espanhol\":\n",
    "        stopwords = nltk.corpus.stopwords.words('spanish')\n",
    "        phrase = []\n",
    "        for word in sentence:\n",
    "            if word not in stopwords and word not in string.punctuation and len(word)>1:\n",
    "                phrase.append(word)\n",
    "        return phrase\n",
    "    elif idioma == \"português\":\n",
    "        stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "        phrase = []\n",
    "        for word in sentence:\n",
    "            if word not in stopwords and word not in string.punctuation and len(word)>1:\n",
    "                phrase.append(word)\n",
    "        return phrase\n",
    "    \n",
    "    \n",
    "#def tokenizar(sentence):\n",
    "#    \"\"\"Função que recebe texto como entrada e devolve lista\n",
    "#    de palavras.\n",
    "#    \"\"\"\n",
    "#    sentence = sentence.lower()\n",
    "#    sentence = nltk.word_tokenize(sentence)\n",
    "#    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função retirada de http://neci-python.blogspot.com/p/blog-page_55.html\n",
    "\"\"\"Lê documento PDF e retorna texto extraido\"\"\"\n",
    "\n",
    "# esta função recebe o caminho do arquivo pdf e retorna seu texto extraido num string\n",
    "def getPDFContent(path):\n",
    "    content = \"\"\n",
    "\n",
    "    # abre o arquivo pdf e cria um objeto reader\n",
    "    f = open(path, 'rb')#insere no python\n",
    "    pdf = PyPDF2.PdfFileReader(f)       # objeto que representa o documento\n",
    "\n",
    "    # Itera pelas páginas do documento\n",
    "    for i in range(0, pdf.getNumPages()):        \n",
    "        # Extrai texto da pagina e apenda no string content\n",
    "        content += pdf.getPage(i).extractText() + \"\\n\"\n",
    "\n",
    "    # fecha o arquivo\n",
    "    f.close()\n",
    "    return content\n",
    "\n",
    "\n",
    "# programa principal, executado ao ativar o módulo\n",
    "print('Extrai texto de arquivo PDF.')\n",
    "print(os.getcwd())\n",
    "print(os.listdir())\n",
    "print('----------------------------')\n",
    "print('Entre nome ou caminho de arquivo PDF:')\n",
    "\n",
    "arq = input()\n",
    "if not arq.endswith('.pdf'):\n",
    "    arq = arq + '.pdf'\n",
    "\n",
    "stx = getPDFContent(arq)\n",
    "\n",
    "txt = stx.split('\\n')\n",
    "\n",
    "print('Texto extraido (primeiros 200 caracteres):')\n",
    "print('-------------------------------------------')\n",
    "print()\n",
    "print(txt[:101])\n",
    "print()\n",
    "print('-------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS))\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-\", \"...\", \"”\", \"”\"]\n",
    "class CleanTextTransformer(TransformerMixin):\n",
    "   def transform(self, X, **transform_params):\n",
    "        return [cleanText(text) for text in X]\n",
    "   def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "def get_params(self, deep=True):\n",
    "        return {}\n",
    "    \n",
    "def cleanText(text):\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = text.lower()\n",
    "    return text\n",
    "def tokenizeText(sample):\n",
    "    tokens = parser(sample)\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printNMostInformative(vectorizer, clf, N):\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    coefs_with_fns = sorted(zip(clf.coef_[0], feature_names))\n",
    "    topClass1 = coefs_with_fns[:N]\n",
    "    topClass2 = coefs_with_fns[:-(N + 1):-1]\n",
    "    print(\"Class 1 best: \")\n",
    "    for feat in topClass1:\n",
    "        print(feat)\n",
    "    print(\"Class 2 best: \")\n",
    "    for feat in topClass2:\n",
    "        print(feat)\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizeText, ngram_range=(1,1))\n",
    "clf = LinearSVC()\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer), ('clf', clf)])\n",
    "# data\n",
    "train1 = train['Title'].tolist()\n",
    "labelsTrain1 = train['Conference'].tolist()\n",
    "test1 = test['Title'].tolist()\n",
    "labelsTest1 = test['Conference'].tolist()\n",
    "# train\n",
    "pipe.fit(train1, labelsTrain1)\n",
    "# test\n",
    "preds = pipe.predict(test1)\n",
    "print(\"accuracy:\", accuracy_score(labelsTest1, preds))\n",
    "print(\"Top 10 features used to predict: \")\n",
    "printNMostInformative(vectorizer, clf, 10)\n",
    "pipe = Pipeline([('cleanText', CleanTextTransformer()), ('vectorizer', vectorizer)])\n",
    "transform = pipe.fit_transform(train1, labelsTrain1)\n",
    "vocab = vectorizer.get_feature_names()\n",
    "for i in range(len(train1)):\n",
    "    s = \"\"\n",
    "    indexIntoVocab = transform.indices[transform.indptr[i]:transform.indptr[i+1]]\n",
    "    numOccurences = transform.data[transform.indptr[i]:transform.indptr[i+1]]\n",
    "    for idx, num in zip(indexIntoVocab, numOccurences):\n",
    "        s += str((vocab[idx], num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(labelsTest1, preds, target_names=df['Conference'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ETAPA_PLANEJAMENTO.csv') as csv_file:\n",
    "    csv_read = csv.reader(csv_file, delimiter=',')\n",
    "      for row in csv_reader:\n",
    "            df[row]= csv_reader[row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\Vanessa Eufrauzino\\\\Documents\\\\PT\\\\PERMISSAOTRABALHO.csv', newline='') as f:\n",
    "    reader = csv.reader(f, delimiter=':', quoting=csv.QUOTE_NONE)\n",
    "    for row in range(10):\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load (\"pt_core_news_sm\")#(\"xx_ent_wiki_sm\")\n",
    "pt=nlp(\"NÃO OBSTRUIR ROTA DE FUGA PARALISAR TRABALHO EM CASO DE EMERGÊNCIA UTILIZAR FERRAMENTA ADEQUADA AO TRABALHO UTILIZAR\"+\n",
    "       \"CINTO DE SEGURANÇA PRESO A ESTRUTURA OU CABO GUIA\")\n",
    "\n",
    "#type(palavras)\n",
    "#tokens = [token for token in textos [1]]\n",
    "\n",
    "tokens2 = [token for token in pt]\n",
    "tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
