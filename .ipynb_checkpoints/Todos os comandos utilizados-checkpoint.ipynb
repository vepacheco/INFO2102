{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yandex_translate\n",
    "import yandex\n",
    "import xmltodict\n",
    "import warnings #para controle de erros e avisos\n",
    "import wand\n",
    "import uuid, json #uui (?) json para leitura de arquivos com essa extensão\n",
    "import time\n",
    "import textract\n",
    "import textpack\n",
    "import tarfile\n",
    "import tabula #transformação de tabelas em PDF para dataframes pandas\n",
    "import sys\n",
    "import string #Manipulação de dados tipo string\n",
    "import statsmodels\n",
    "import sqlite3\n",
    "import spacy #NLP e Machine learning\n",
    "import sklearn.feature_extraction\n",
    "import shutil #para copia de arquivos dentro do python. Mas não copia metadados\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import requests #integração de recursos web no codigo, como api de leitura e envio de dados\n",
    "import re #modulo regex, de expressões regulares\n",
    "import pytesseract as ocr\n",
    "import PyPDF2 #leitor de pdf e transforma em objetos python\n",
    "import prince as pc\n",
    "import pickle\n",
    "import pandas as pd #leitor de conjunto de dados como um dataframe e criador de estatisticas dos dados\n",
    "import os #para funcionalidade do SO como abrir arquivo e caminho e também para exceções\n",
    "import numpy as np #pacote para a computação científica; leitura de array e matrizes, alem de operações matemáticas embutidas\n",
    "import nltk #processamento de linguagem natural como criação de tokens e entidades\n",
    "import multiprocessing as mp\n",
    "import matplotlib.pyplot as plt #pct para geração de graficos\n",
    "import math\n",
    "import locale\n",
    "import io #serve para lidar com vários tipos de E/S\n",
    "import googletrans\n",
    "import glob\n",
    "import gensim\n",
    "import csv #leitura e criação de arquivos csv e transformação em objeto python\n",
    "import codecs\n",
    "import click #criação de interface mais bonitinha e elegante\n",
    "import base64\n",
    "from yandex_translate import YandexTranslate\n",
    "from yandex import Translater\n",
    "from xgboost import XGBClassifier\n",
    "from wand.image import Image, Color\n",
    "from wand.exceptions import CorruptImageError\n",
    "from tqdm import tqdm_notebook\n",
    "from tika import parser\n",
    "\n",
    "\n",
    "from tabula import read_pdf\n",
    "from string import punctuation\n",
    "from spacy.lang.pt import pt_core_news_sm\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang import pt_core_news_sm\n",
    "from smart_open import smart_open\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer #Machine Learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer #Machine Learning\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import utils\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "from PyPDF2 import PdfFileWriter, PdfFileReader\n",
    "from PIL import Image\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer import (layout as pdfminer_layout, high_level as pdfminer_high_level)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from multiprocessing import Pool\n",
    "from io import StringIO\n",
    "from googletrans import Translator\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from flask.cli import with_appcontext\n",
    "from flask import current_app, g\n",
    "from collections import Counter\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "stop_words = stopwords.words('portuguese') + list(punctuation) + stopwords.words('english')\n",
    "parser = English()\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download ()\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comandos python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_rows\", None) #Comando para não restringir o numero de linhas ou colunas impressas na telas.\n",
    "tqdm_notebook() #barra de progresso\n",
    "df.isdigit()#saber se um objeto é numero\n",
    "df.drop('coluna', inplace = True, axis = 1)# Remover uma coluna ou uma linha de um objeto pandas\n",
    "df1=df1.dropna(subset =['ETPL_TX_DESCRICAO','PONDERACAO'])\n",
    "df.drop_duplicates(subset =\"coluna\", keep = False, inplace = True)\n",
    "pd.crosstab\n",
    "df = pd.DataFrame({'nome da coluna': valores}) # Transformando uma lista em dataframe pandas\n",
    "set(temp.split(\",\")#transforma em conjunto unitário e ordenado\n",
    "set(1).union(set(2))\n",
    "set(1).intersection(set(2))# Funções de união e interseção de conjuntos\n",
    "df3=pd.merge(df1,df2,how=\"left\", on=[\"Group\"])\n",
    "len(objeto) #SABENDO TAMANHO DO ARQUIVO\n",
    "lista.append(dado)#para acrescentar um novo dado na lista\n",
    "df.sort_index(by='coluna')#para ordenar por determinada coluna\n",
    "lista.update(dado a atualizar a lista)\n",
    "df.value_counts()\n",
    "df.head()\n",
    "df.reset_index()\n",
    "pd.concat('dataframes a serem juntados', axis=0, join='outer', ignore_index=True, keys=None,levels=None, names=None, verify_integrity=False, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_STARTPOINT='https://api-pai-dev.transformacaodigitalspassu.com.br/api/pai/getetapas?cdPlat=P-77'\n",
    "recebidos=requests.get(url=API_STARTPOINT)#'serv_idServico', 'serv_nrAno', 'cdEtapa',  'plat_cdPlataf' 'txDescricao'\n",
    "dfP77=recebidos.json()\n",
    "API_ENDPOINT='https://api-pai-dev.transformacaodigitalspassu.com.br/api/pai/registraalerta/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    <condição>\n",
    "except Exception:\n",
    "    #erros.append([etapa['ETPL_TX_DESCRICAO'],i])\n",
    "    continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abrindo e salvando um objeto em planilha .csv ou .xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv_ou_excel(\"endereço e nome do arquivo\", sep=';', error_bad_lines=False, encoding = ['latin-1', 'utf-8'], usecols = ['quais colunas'])\n",
    "df.to_csv_ou_excel(\"endereço e nome do arquivo\", sep = ';', skipfooter = nº de linhas que são rodapé, engine = \"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo os dados em PDF do diretório"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivos = [arq for arq in os.listdir(os.getcwd()) if arq.lower().endswith(\".pdf\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outro meio de abrir dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_documents(root):\n",
    "    docs = []\n",
    "    for r, dirs, files in os.walk(root):\n",
    "        for file in files:\n",
    "            with open(os.path.join(r, file), \"r\") as f:\n",
    "                docs.append(f.read())     \n",
    "    return dict([('docs', docs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraindo dados de PDF para planilha csv usando TABULA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tabula.convert_into_by_batch(os.getcwd(), output_format='csv')\n",
    "#df=[]\n",
    "#for i in tqdm_notebook(os.getcwd()):\n",
    "    df.append(tabula.read_pdf(i))\n",
    "#tabula.convert_into(\"2018_12_29_SRGE_P68_ALERTA_Queda passarela da gangway.pdf\", \"output.csv\", output_format=\"csv\")\n",
    "#converte apenas um em csv\n",
    "#df=pd.read_csv(\"2018_12_29_SRGE_P68_ALERTA_Queda passarela da gangway.csv\",encoding=\"cp1250\")\n",
    "#df.head\n",
    "#obj=df['ALERTA DE SMS No: PB 001/2019'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraindo o texto do arquivo em PDF para um objeto python com parser TIKA e PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos_alertas=[]\n",
    "nomes=[]\n",
    "erros=[]\n",
    "for i in tqdm_notebook(range(len(arquivos))):\n",
    "    try:\n",
    "        temp= parser.from_file('C:\\\\Users\\\\Vanessa Eufrauzino\\\\Documents\\\\alertas-portugues\\\\pdf\\\\'+ arquivos[i])['content']\n",
    "        temp = temp.lower()\n",
    "        temp = temp.replace(\"\\n\",\" \")\n",
    "        nomes.append(arquivos[i])\n",
    "        textos_alertas.append(temp)\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            temp = open('C:\\\\Users\\\\Vanessa Eufrauzino\\\\Documents\\\\alertas-portugues\\\\pdf\\\\'+ arquivos[i], 'rb')#insere no python\n",
    "            pdf = PyPDF2.PdfFileReader(temp)#cria objeto \n",
    "            for j in range (pdf.getNumPages()):\n",
    "                textos = pdf.getPage(j).extractText()#retira o texto\n",
    "                textos = textos.lower()\n",
    "                textos = textos.replace(\"\\n\",\" \")\n",
    "                textos = textos.replace(\"   \",\" \")\n",
    "                nomes.append(arquivos[i])\n",
    "                textos_alertas.append(textos)\n",
    "        except Exception as e:\n",
    "              erros.append([e,i])\n",
    "        continue  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraindo o texto de uma imagem para um objeto python com tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm_notebook(range(len(indices))):\n",
    "    textos = (textract.process('C:\\\\Users\\\\Vanessa Eufrauzino\\\\Documents\\\\alertas-portugues\\\\pdf\\\\'+ arquivos[indices[i]], method = 'tesseract')).decode(\"utf-8\")\n",
    "    textos = textos.lower()\n",
    "    textos = textos.replace(\"\\n\",\" \")\n",
    "    textos = textos.replace(\"\\r\",\" \")\n",
    "    dfalertas['conteudo'][indices[i]] = textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de conversão de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "def is_date(string, fuzzy=False):\n",
    "    \"\"\"\n",
    "    Return whether the string can be interpreted as a date.\n",
    "    :param string: str, string to check for date\n",
    "    :param fuzzy: bool, ignore unknown tokens in string if True\n",
    "    \"\"\"\n",
    "    try: \n",
    "        parse(string, fuzzy=fuzzy)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "    \n",
    "import dateutil.parser\n",
    "from dateutil.parser import parse\n",
    "datetime = parse(\"04/06/00\", dayfirst = True)\n",
    "print(datetime.date())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função limpa texto Renan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##' '.join([word for word in temp.split(\" \") if word not in stop_words] -->limpar stopwords\n",
    "\n",
    "def limpa_texto(texto, b):\n",
    "    try:\n",
    "        while (texto[0] in b):\n",
    "            texto = texto[1:]\n",
    "    except:\n",
    "        texto = texto\n",
    "    try:\n",
    "        while (texto[-1] in b):\n",
    "            texto = texto[:-1]\n",
    "    except:\n",
    "        texto = texto\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de composição de frases para perfil de risco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partes_corpo=list(['mão','pé','cabeça','olhos','dedos','bracos','pernas','boca','costela',\n",
    "                 'orelha','joelhos','cabelos','testa','pele','coluna'])\n",
    "\n",
    "implicação=list(['primeiros socorros','tratamento medico','restrição de atividades',\n",
    "               'incapacidade temporaria parcial/total','incapacidade permanente parcial/total','dano pessoal','morte'])\n",
    "\n",
    "natureza_lesao=list(['contusão','esmagamento','lesão imediata','corte','laceração','ferida','punctura','queimadura','resvaladura',\n",
    "                   'escaldadura','distensão','torção','luxação','escoriação','abrasão','corte','outras lesões','fratura'])\n",
    "\n",
    "agente_causador= list(['queda','acessórios','acido','acoplamento','acumuladores','aeronave','agente','agitador',\n",
    "                       'agua','alavanca','álcalis','álcool','alicate','alimentícios','amônia','andaime','animal',\n",
    "                       'aparafusar','aparas','aquecedor','aquecimento','ar','arame','arco','areia','armário',\n",
    "                       'armazenamento','aromático','arquivo','asfalto','balcão','bancada','bancos','barramento',\n",
    "                       'barrica','barril','batedeira','bateria','benzeno','bicicleta','bomba','britador', \n",
    "                       'broqueadeira','bruto', 'cabo','cabra','caçambas','cadeiras','cais','caixa','caixão',\n",
    "                       'caixote','calandra','calcada','calcado','caldeira','calor','câmara','caminhão',\n",
    "                       'caminhonete','canal','canaleta','capacho','carbônico','carbono','carga','carreta',\n",
    "                       'carro','carvão','cavadeira','cerâmica','chama','chão','chapa','chave','cilindro',\n",
    "                       'cilindros','cinzel','cloro','combustível','composto','compostos','compressor', \n",
    "                       'comprimido','construção','conversor','copiadora','coque','corda','correias','corrente', \n",
    "                       'cortar','degraus','descarga','descompressão','desmontável','detergente','diesel','disjuntor',\n",
    "                       'projeção','divisória','doca','edificações','edifício ','elétrica','elétrico','elevada', \n",
    "                       'elevado','elevador','embalar','embarcação','embreagem','emissor','empacotar','empilhadeira',\n",
    "                       'energia','engradado','engrenagem','entulho', 'enxada','epi','equipamento','escada', \n",
    "                       'escafandro','escavação','escavações','escritório','esferas','esmeril', 'esmerilhadeira',\n",
    "                       'estante','estrada','estradas','estria','estrutura','estruturas','explosivo','extensível',\n",
    "                       'faca','facão','ferramenta','ferramentas','fibra','fichário','fogão','forca','forcado', \n",
    "                       'forjar','forno','forração','fosso','fossos','fragmentos','frasco','fricção','fundido',\n",
    "                       'fundir','furadeira','fusível','galeria','garfo','ancinho','garrafa','gás','gasolina',\n",
    "                       'gerador','globo','gravidade','graxa','grifo','guilhotina',   'guincho','guindar',\n",
    "                       'guindaste','halogenados','hidráulico','iluminação','impacto','impressora','incandescente',\n",
    "                       'infeccioso','inglesa','interruptor','ionizante','isolantes','janela','jato','lâmina',\n",
    "                       'laminadora','lâmpada','lima','limpeza','liquefeito','liquido','líquidos','lixadeira',\n",
    "                       'lubrificante','luminária','macaco','maçarico', 'machadinha','machado','madeira','malho',\n",
    "                       'mangueira','manilhas','maquina','marítima','marreta','martelete','martelo','materiais',\n",
    "                       'material','mecânica','mecânico','medicamentos','mergulho','mesa','metal','metálico',\n",
    "                       'metálicos','mina','mineração','minerais','misturador','mobiliário','moinho','monóxido',\n",
    "                       'motocicleta','motoneta','motor','motriz','nafta','nitrogênio','óleo','órgãos','pá',\n",
    "                       'painel','papel','parafina','parafuso','parasitário','partículas','passarela','pau',\n",
    "                       'pedra','peneira','perfuração','perfuratriz','petróleo','picareta','píer','piso',\n",
    "                       'placa','plaina','plásticos','plataforma','pneumático','poço','polia','politriz',\n",
    "                       'ponte','ponteiro','porca','porta','portátil','pórtico','poste','pranchão','prego',\n",
    "                       'prensa','pressão','processo','produto','produtos','punção','quente', 'querosene','química',\n",
    "                       'quina', 'radiação','radioisótopo','raio','rebitadeira','rebite','recipientes','refratários',\n",
    "                       'reostato','resfriadores','resfriamento','retificador','ripa','rolante','roldana','rua',\n",
    "                       'ruído','sabão','separador','serra','serrote','sino','socador','soldagem','soldar',\n",
    "                       'solventes','sonda','substancia','substancias','sucata','sulfídrico','sustentação','tabua',\n",
    "                       'talha','talhadeira','tambor','tanque','tapete','telhado','telhas','tenaz','terraplenagem',\n",
    "                       'tesoura','tesourão','têxteis','tijolos','tolueno','torques','torre','torres','tração','tranca',\n",
    "                       'transformador','transmissão','transportador','trator','trilho','trilhos','trocadores','tubo','tubos',\n",
    "                       'tubulação','tupia','turbina','válvula','vapor','vaso','vasos','vazador','veiculo','vergalhão','verruma',\n",
    "                       'viaduto','vidraria','vidro','xileno','pé de cabra'])\n",
    "\n",
    "def frases_composicao(caracteristicas): \n",
    "    frase1=\"Região mais afetada: \" + str([word for word in caracteristicas if word in partes_corpo])\n",
    "    frase2=\"Maior ocorrência de: \" + str([word for word in caracteristicas if word in natureza_lesao])\n",
    "    frase3=\"Como maior consequência: \" + str([word for word in caracteristicas if word in implicação])\n",
    "    frase4=\"Sendo causado por :\" + str([word for word in caracteristicas if word in agente_causador])\n",
    "    frase1=frase1.replace('[', \"\")\n",
    "    frase2=frase2.replace('[', \"\")\n",
    "    frase3=frase3.replace('[', \"\")\n",
    "    frase4=frase4.replace('[', \"\")\n",
    "    frase1=frase1.replace(']', \"\")\n",
    "    frase2=frase2.replace(']', \"\")\n",
    "    frase3=frase3.replace(']', \"\")\n",
    "    frase4=frase4.replace(']', \"\")\n",
    "    return(list([frase1, frase2, frase3, frase4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridade de cosseno manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_all(text):\n",
    "    \"\"\"\n",
    "    Armazena em um vetor todas as palavras dos textos sem repetições.\n",
    "    \"\"\"\n",
    "    text_set = set()\n",
    "    for w in [words.split() for words in text]:\n",
    "        text_set.update(w)\n",
    "    return np.array(list(text_set))\n",
    "\n",
    "\n",
    "vocabulario = text_all(dfalertas['conteudo'])\n",
    "vocabulario2 = text_all(dfetap['ETPL_TX_DESCRICAO'])\n",
    "\n",
    "def fit_transform(text, words=vocabulario):\n",
    "    \"\"\"\n",
    "    Converte o texto em um vetor, onde compara se cada palavra obtida no vetor de \n",
    "    todas as palavras contém ou não em cada texto. \n",
    "    Insere 1 se sim e 0 se não.\n",
    "    \"\"\"\n",
    "    #return [1 if word in text.split() else 0 for word in words]\n",
    "    return [int(word in text.split()) for word in words]\n",
    "\n",
    "features = np.array(list(map(fit_transform, dfalertas['conteudo'])))\n",
    "\n",
    "len(features)\n",
    "\n",
    "def fit_transform(text, words=vocabulario):\n",
    "    \"\"\"\n",
    "    Converte o texto em um vetor, onde compara se cada palavra obtida no vetor de \n",
    "    todas as palavras contém ou não em cada texto. \n",
    "    Insere 1 se sim e 0 se não.\n",
    "    \"\"\"\n",
    "    #return [1 if word in text.split() else 0 for word in words]\n",
    "    return [int(word in text.split()) for word in words]\n",
    "\n",
    "features2 = np.array(list(map(fit_transform, dfetap['ETPL_TX_DESCRICAO'])))\n",
    "\n",
    "\n",
    "def cosine_similarity(v, w):\n",
    "    return np.dot(v, w)/np.sqrt(np.dot(v, v)*np.dot(w, w))    \n",
    "    #return np.dot(v, w)/(np.linalg.norm(v)*np.linalg.norm(w))\n",
    "\n",
    "def text_simillarities(id_text, text, features=features, title=dfalertas['Arquivo'], n_text=len(features)):\n",
    "    \"\"\"\n",
    "    Dado o texto a ser analisado, a função retorna em ordem descrecente quais os demais textos são\n",
    "    similares ao analisado. A função retorna matriz de 2 por n_text, onde a primeira e a segunda coluna\n",
    "    refere-se ao texto analisado e a similaridade do texto analisado, respectivamente.\n",
    "    \"\"\"\n",
    "    simillarity = [[cosine_similarity(features2[id_text], feature), int(i)] for i, feature in enumerate(features)]\n",
    "    simillarity = np.array(sorted(simillarity, key=lambda sim: sim[0], reverse=True))    \n",
    "    return [[title[y], text, simillarity[x, 0]] for x, y in enumerate(np.int0(simillarity[1:,1]), 1)][:n_text]\n",
    "\n",
    "similaridade = []\n",
    "for etapa in tqdm_notebook(range(len(dfetap['ETPL_TX_DESCRICAO']))):\n",
    "    for x,t, s in text_simillarities(etapa, dfetap['ETPL_TX_DESCRICAO'][etapa]):\n",
    "         similaridade.append([x, t, round(s, 8)])\n",
    "            \n",
    "                     \n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "features = vectorizer.fit_transform(textos).todense()\n",
    "def text_simillarities_2(id_text, features=features, text=textos, n_text=3):\n",
    "    \"\"\"\n",
    "    Dado o texto a ser analisado, a função retorna em ordem descrecente quais os demais textos são\n",
    "    similares ao analisado. A função retorna matriz de 2 por n_text, onde a primeira e a segunda coluna\n",
    "    refere-se ao texto analisado e a similaridade do texto analisado, respectivamente.\n",
    "    \"\"\"\n",
    "    simillarity = [[cosine_similarity(features[id_text], feature)[0,0], int(i)] for i, feature in enumerate(features)]\n",
    "    simillarity = np.array(sorted(simillarity, key=lambda sim: sim[0], reverse=True))    \n",
    "    return [[text[y], simillarity[x, 0]] for x, y in enumerate(np.int0(simillarity[1:,1]), 1)][:n_text]\n",
    "\n",
    "print('Texto analisado -> ',textos[3], '\\n')\n",
    "for t, s in text_simillarities_2(3):\n",
    "    print('Texto: {} | Similaridade: {}'.format(t, round(s, 4)))\n",
    "#text_simillarities_2(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridade encontrada na internet (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testando mais uma similaridade de cosseno com bigram\n",
    "\n",
    "import nltk \n",
    "import re\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "from unicodedata import normalize\n",
    "from nltk import ngrams\n",
    "\n",
    "#Regex para encontrar tokens\n",
    "REGEX_WORD = re.compile(r'\\w+')\n",
    "#Numero de tokens em sequencia\n",
    "N_GRAM_TOKEN = 3\n",
    "\n",
    "#Faz a normalizacao do texto removendo espacos a mais e tirando acentos\n",
    "def text_normalizer(src):\n",
    "    return re.sub('\\s+', ' ',\n",
    "                normalize('NFKD', src)\n",
    "                   .encode('ASCII','ignore')\n",
    "                   .decode('ASCII')).lower().strip()\n",
    "\n",
    "#Faz o calculo de similaridade baseada no coseno\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        coef = float(numerator) / denominator\n",
    "        if coef > 1:\n",
    "            coef = 1\n",
    "        return coef\n",
    "\n",
    "#Monta o vetor de frequencia da sentenca\n",
    "def sentence_to_vector(text, use_text_bigram):\n",
    "    words = REGEX_WORD.findall(text)\n",
    "    accumulator = []\n",
    "    for n in range(1,N_GRAM_TOKEN):\n",
    "        gramas = ngrams(words, n)\n",
    "        for grama in gramas:\n",
    "            accumulator.append(str(grama))\n",
    "\n",
    "    if use_text_bigram:\n",
    "        pairs = get_text_bigrams(text)\n",
    "        for pair in pairs:\n",
    "            accumulator.append(pair)\n",
    "\n",
    "    return Counter(accumulator)\n",
    "\n",
    "#Obtem a similaridade entre duas sentencas\n",
    "def get_sentence_similarity(sentence1, sentence2, use_text_bigram=False):\n",
    "    vector1 = sentence_to_vector(text_normalizer(sentence1), use_text_bigram)\n",
    "    vector2 = sentence_to_vector(text_normalizer(sentence2), use_text_bigram)\n",
    "    return cosine_similarity(vector1, vector2)\n",
    "\n",
    "#Metodo de gerar bigramas de uma string\n",
    "def get_text_bigrams(src):\n",
    "    s = src.lower()\n",
    "    return [s[i:i+2] for i in range(len(s) - 1)]\n",
    "\n",
    "result = []\n",
    "similaridades=[]\n",
    "for i in tqdm_notebook(range(10000)):   \n",
    "    if __name__ == \"__main__\":\n",
    "        w1 = dfetap['texto_concat'][i]\n",
    "        words = textos \n",
    "\n",
    "        #print('Busca: ' + w1)\n",
    "\n",
    "        #Nivel de aceite (40%)\n",
    "        #cutoff = 0.40\n",
    "        #Sentenças similares\n",
    "        \n",
    "        for w2 in words:\n",
    "            #print('\\nCosine Sentence --- ' + w2)\n",
    "\n",
    "            #Calculo usando similaridade do coseno com apenas tokens\n",
    "            similarity_sentence = get_sentence_similarity(w1, w2)\n",
    "            #print('\\tSimilarity sentence: ' + str(similarity_sentence))\n",
    "\n",
    "            #Calculo usando similaridade do coseno com tokens e com ngramas do texto\n",
    "            #similarity_sentence_text_bigram = get_sentence_similarity(w1, w2, use_text_bigram=True)\n",
    "            #print('\\tSimilarity sentence: ' + str(similarity_sentence_text_bigram))\n",
    "            similaridades.append((w1,w2,str(similarity_sentence)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridade por spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "nlp = spacy.load (\"pt_core_news_sm\")#(\"xx_ent_wiki_sm\")\n",
    "palavras = nlp(textos[1])\n",
    "#type(palavras)\n",
    "tokens = [token for token in palavras]\n",
    "tokens\n",
    "# Process whole documents\n",
    "        # Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in palavras.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in palavras if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "#for entity in palavras.ents:\n",
    "#    print(entity.text, entity.label_)\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)    \n",
    "    \n",
    "#displacy.render(palavras, style='dep', jupyter=True, options={'distance': 90}) \n",
    "\n",
    "\n",
    "\n",
    "nlp = spacy.load (\"pt_core_news_sm\")#(\"xx_ent_wiki_sm\")\n",
    "cont=0\n",
    "cont2=0\n",
    "with open('similaridadefile2.csv','w+') as similaridade2_file:\n",
    "    similaridade2_writer = csv.writer(similaridade2_file, delimiter=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for sms in tqdm_notebook(textos):\n",
    "        sms_i = nlp(sms)\n",
    "        for n in c[0:100]:\n",
    "            similaridade2_writer.writerow([nome_arq[cont],cd_pt[cont2],cd_plat[cont2], nlp(n), sms_i.similarity(nlp(n))]) \n",
    "            cont2+=1\n",
    "        cont+=1 \n",
    "        \n",
    "\n",
    "        \n",
    "def funcao_parallel (sms):\n",
    "#    for sms in textos:\n",
    "    sms_i = nlp(sms)\n",
    "    tokens = [token for token in sms_i]\n",
    "    for n in pt:\n",
    "        pt_i = nlp(n)\n",
    "        tokens2 = [token for token in pt_i]\n",
    "        cont = 0\n",
    "        for token in range(len(tokens)):\n",
    "             for token2 in range (len(tokens2)):\n",
    "                cont=cont + abs(tokens[token].similarity(tokens2[token2]))\n",
    "    return cont            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similaridade por textpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grupo = tp.TextPack(df,[\"colunas a serem usadas para o agrupamento\"], match_threshold=0.75, ngram_remove=r'[,-./;]', ngram_length = 5)\n",
    "grupo.run()   \n",
    "grupo.export_csv('endereço/nome_do_arquivo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk = teste.groupby('PLAT_CD_PLATAF') \n",
    "df['Group'].unique()\n",
    "z=gk.get_group('PNA2').\n",
    "z=z.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_marks(nrows, chunk_size):\n",
    "    return range(1 * chunk_size, (nrows // chunk_size + 1) * chunk_size, chunk_size)\n",
    "\n",
    "indices = list(index_marks(etapa.shape[0], 1000000))\n",
    "print(\"Marks: {}\".format(indices))\n",
    "\n",
    "def split(df, chunk_size):\n",
    "    indices = index_marks(df.shape[0], chunk_size)\n",
    "    return np.split(df, indices)\n",
    "\n",
    "chunks = split(etapa, 1000000)\n",
    "for c in chunks:\n",
    "    print(\"Shape: {}; {}\".format(c.shape, c.index))\n",
    "\n",
    "#chunks[0].to_csv('./risco/df1.csv', encoding=\"latin-1\", sep=';')\n",
    "chunks[1].to_csv('./risco/df2.csv', encoding=\"latin-1\", sep=';')\n",
    "chunks[2].to_csv('./risco/df3.csv', encoding=\"latin-1\", sep=';')\n",
    "chunks[3].to_csv('./risco/df4.csv', encoding=\"latin-1\", sep=';')\n",
    "chunks[4].to_csv('./risco/df5.csv', encoding=\"latin-1\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tradução de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modulo google funcionou para a tradução de um texto complelo\n",
    "import googletrans\n",
    "from googletrans import Translator\n",
    "tr=Translator()\n",
    "teste=tr.translate(texto[1],dest='en')\n",
    "print(teste.text)\n",
    "\n",
    "#import goslate ### funciona, não sei se tem limites... vou testar (https://pythonprogramminglanguage.com/translate/) #tem limite\n",
    "#texto = arquivos[1]\n",
    "#gs = goslate.Goslate()\n",
    "#translatedText = gs.translate(texto,'en')\n",
    "#print(translatedText) \n",
    "###############tentativa modulo ibm #######\n",
    "#from gpclient import GPClient, GPServiceAccount, GPTranslations ### da ibm (https://pypi.org/project/gp-python-client/1.0.0/)\n",
    "#import locale ### da ibm #abortada a tentativa, não funciona\n",
    "\n",
    "#tradução com outro modulo\n",
    "teste = YandexTranslate('trnsl.1.1.20190710T130644Z.2721d78249c2ffe1.1c235bb20a21b3f087104d104ce27fa40f756101')\n",
    "traduction =('Translate:', teste.translate(texto, 'pt-en'))\n",
    "print(traduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "def sml(a,b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "print(sml(a[1],b[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Teste de modelos \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(risco['Etapa'],risco[\"Risco1\"], test_size=0.2,random_state=45)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "y_train_counts = count_vect.fit_transform(y_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "y_train_tfidf = tfidf_transformer.fit_transform(y_train_counts)\n",
    "\n",
    "clfM = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "\n",
    "clfL = LinearSVC().fit(X_train_tfidf, y_train)\n",
    "\n",
    "clfRFC = RandomForestClassifier().fit(X_train_tfidf, y_train)\n",
    "\n",
    "clfLR = LogisticRegression().fit(X_train_tfidf, y_train)\n",
    "\n",
    "accuracy_score(y_test,preditosRFC(X_test))\n",
    "\n",
    "def preditosRFC(treino):    \n",
    "    valoresclfRFC=[]\n",
    "    for i in tqdm_notebook(range(len(treino))):\n",
    "        valoresclfRFC.append(clfRFC.predict(count_vect.transform([treino[treino.index[i]]])))\n",
    "    return (valoresclfRFC)\n",
    "\n",
    "def preditosLR(treino):\n",
    "    valoresclfLR=[]\n",
    "    for i in tqdm_notebook(range(len(treino))):\n",
    "        valoresclfLR.append(clfLR.predict(count_vect.transform([treino[treino.index[i]]])))\n",
    "    return (valoresclfLR)\n",
    "\n",
    "def preditosM(treino):    \n",
    "    valoresclfM=[]\n",
    "    for i in tqdm_notebook(range(len(treino))):\n",
    "        valoresclfM.append(clfM.predict(count_vect.transform([treino[treino.index[i]]])))\n",
    "    return (valoresclfM)\n",
    "\n",
    "def preditosL(treino):    \n",
    "    valoresclfL=[]\n",
    "    for i in tqdm_notebook(range(len(treino))):\n",
    "        valoresclfL.append(clfL.predict(count_vect.transform([treino[treino.index[i]]])))\n",
    "    return (valoresclfL)\n",
    "\n",
    "print(accuracy_score(y_test,preditosRFC(X_test)),'\\n',\n",
    "      accuracy_score(y_test,preditosLR(X_test)),'\\n',\n",
    "      #accuracy_score(y_test,preditosM(X_test)),'\\n',\n",
    "      accuracy_score(y_test,preditosL(X_test)))\n",
    "\n",
    "conj_risco2=[]\n",
    "erros=[]\n",
    "for i in tqdm_notebook(range(len(pna1_2_p77['ETPL_TX_DESCRICAO']))):\n",
    "    try:\n",
    "        conj_risco2.append([pna1_2_p77['PLAT_CD_PLATAF'][i],pna1_2_p77['SERV_CD_SERVICO'][i],pna1_2_p77['SERV_NR_ANO'][i],\n",
    "                          pna1_2_p77['ETPL_CD_ETAPA'][i],pna1_2_p77['ETPL_TX_DESCRICAO'],clfL.predict(count_vect.transform([pna1_2_p77['ETPL_TX_DESCRICAO'][i]]))])\n",
    "    except Exception:\n",
    "        erros.append([pna1_2_p77['ETPL_TX_DESCRICAO'],i])\n",
    "        continue     \n",
    "\n",
    "\n",
    "#colunas=list(['PLAT_CD_PLATAF','SERV_CD_SERVICO','SERV_NR_ANO','ETPL_CD_ETAPA','ETPL_TX_DESCRICAO','RISCO'])\n",
    "#df['RISCO'] = pd.DataFrame(conj_risco2, columns = colunas)\n",
    "\n",
    "# risco=[]\n",
    "# frases=[]\n",
    "# for v in tqdm_notebook(range(len(df['RISCO']))):\n",
    "#     temp = df[\"RISCO\"][v][0].split(\" - \")\n",
    "#     temp[1] = temp[1].replace(', ',',')\n",
    "#     risco.append(temp[0])\n",
    "#     if (temp[1] == 'Não existe caracteristica de acidente relacionada a essa etapa'):\n",
    "#         frases.append('Não existe caracteristica de acidente relacionada a essa etapa')\n",
    "#     else:\n",
    "#         z=temp[1].split(',')\n",
    "#         frases.append(frases_composicao(z))\n",
    "\n",
    "#df['riscos'] = risco\n",
    "#df['frases'] = frases\n",
    "#df\n",
    "df.to_csv(\"./Nova pasta/frases_pna1_2_p77.csv\", sep=\";\", encoding='latin-1')\n",
    "for i in tqdm_notebook(range(len(pna1_2['ETPL_TX_DESCRICAO']))):\n",
    "        data = {'ETPL_CD_ETAPA': pna1_2[ 'ETPL_CD_ETAPA'][i],\n",
    "               'PLAT_CD_PLATAF': pna1_2['PLAT_CD_PLATAF'][i],\n",
    "               'SERV_NR_ANO': pna1_2['SERV_NR_ANO'][i],\n",
    "               'SERV_CD_SERVICO': pna1_2['SERV_CD_SERVICO'][i],\n",
    "               'Risco': pna1_2['riscos'][i],\n",
    "               'Composicao': pna1_2['caracteristicas'][i]}\n",
    "        r=requests.post(url = endpoint_post, data = data)\n",
    "        print(r)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model = XGBClassifier()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# clrTree = tree.DecisionTreeClassifier()\n",
    "# clrTree = clrTree.fit(X_train, y_train)\n",
    "# outTree = clrTree.predict(X_test)\n",
    "# clrKN = KNeighborsClassifier()\n",
    "# clrKN = clrKN.fit(X_train, y_train)\n",
    "# outKN = clrKN.predict(X_test)\n",
    "\n",
    "# count_vect = CountVectorizer()\n",
    "# X_test_counts = count_vect.fit_transform(X_test)\n",
    "# tfidf_transformer = TfidfTransformer()\n",
    "# X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)\n",
    "\n",
    "# knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "# knn.fit(X_train_tfidf,y_train)\n",
    "\n",
    "# knn.score(X_test_tfidf, y_test)\n",
    "\n",
    "# modelo=linear_model.LinearRegression()\n",
    "# #train model\n",
    "# modelo.fit(X_train_tfidf, y_train)\n",
    "teste=open('skip_s1000.txt', 'r', encoding='latin-1') \n",
    "for linha in teste:\n",
    "    print(linha)\n",
    "arquivo.close()\n",
    "\n",
    "def escrever_txt(lista):\n",
    "    with open('meu_arquivo.txt', 'w', encoding='utf-8') as f:\n",
    "        for nome in lista:\n",
    "            f.write(nome + '\\n')\n",
    "#textos_array=np.array(textos)\n",
    "#textos_array\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "y = np.array([1, 1, 2, 2])\n",
    "pred = np.array([0.1, 0.4, 0.35, 0.8]) \n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2))\n",
    "maisum = tfidf.fit_transform(c)\n",
    "#labels = df.category_id\n",
    "maisum.shape\n",
    "#TfidfVectorizer \n",
    "\n",
    "from gensim.models import FastText\n",
    "model = KeyedVectors.load_word2vec_format('cbow_s300.txt')\n",
    "\n",
    "print(model.most_similar('copo'))\n",
    "\n",
    "# from gensim.models import FastText  # FIXME: why does Sphinx dislike this import?\n",
    "from gensim.test.utils import common_texts  # some example sentences\n",
    "   print(common_texts[0])\n",
    "\n",
    "print(len(common_texts))\n",
    "\n",
    "model = FastText(size=4, window=3, min_count=1)  # instantiate\n",
    "model.build_vocab(sentences=common_texts)\n",
    "model.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)  # train\n",
    "\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "assert 'teach' == lancaster_stemmer.stem('teacher') == lancaster_stemmer.stem('teaches')\n",
    "\n",
    "lancaster_stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################inicio ultimo risco feito #######################################\n",
    "mais_palavras=['alerta', 'regra', 'ouro', 'ocorrencia', 'utilizados', 'palavras', 'apenas', 'acidentes','nº','preliminar','SMS', \n",
    "              'cuidados', 'sugeridos', 'aplicada', 'foto', 'fotos', 'data', 'emissão', 'atividade', 'presença', 'sms',\n",
    "              'devemos', 'evitar', 'palavra', 'alertas', 'possível', 'possíveis', '/' ,'cob./', 'mod./', 'mm./','`'] \n",
    "numeros=['0','1','2','3','4','5','6','7','8','9','2015','2016','2017','2018','01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "stop_words = stopwords.words('portuguese') + list(punctuation) + mais_palavras + stopwords.words('english')+ numeros\n",
    "\n",
    "API_STARTPOINT='https://api-pai-dev.transformacaodigitalspassu.com.br/api/pai/getetapas?cdPlat=P-77'\n",
    "\n",
    "recebidos=requests.get(url=API_STARTPOINT)#'serv_idServico', 'serv_nrAno', 'cdEtapa',  'plat_cdPlataf' 'txDescricao'\n",
    "dfP77=recebidos.json()\n",
    "\n",
    "lista=[]\n",
    "erros=[]\n",
    "for i in tqdm_notebook(range(len(dfP77))):\n",
    "    temp=dfP77[i]['txDescricao']\n",
    "    temp=temp.lower()\n",
    "    temp=temp.replace('\\n', \"\")\n",
    "    temp=temp.replace(';', \"\")\n",
    "    lista.append([' '.join([word for word in temp.split(\" \") if word not in stop_words]), dfP77[i]['plat_cdPlataf'],\n",
    "                  dfP77[i]['serv_nrAno'], dfP77[i]['cdEtapa'], dfP77[i]['serv_idServico']])\n",
    "\n",
    "colunas=list(['txDescricao', 'plat_cdPlataf', 'serv_nrAno', 'cdEtapa', 'serv_idServico'])\n",
    "dfpna1=pd.DataFrame(lista,columns=colunas)\n",
    "dfpna1['Risco']=0\n",
    "\n",
    "risco=pd.read_csv('./Nova pasta/risco_final.csv', sep=\",\", encoding='utf-8')\n",
    "risco.drop(['Unnamed: 0','Quantidade_carac_acidentes'],axis=1, inplace = True)\n",
    "\n",
    "y=[]\n",
    "for i in tqdm_notebook(range(len(risco[\"Risco\"]))):\n",
    "    y.append(str(risco[\"Risco\"][i]) + \" - \" + risco[\"Caracteristicas\"][i])\n",
    "        \n",
    "risco[\"Risco1\"]=y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(risco['Etapa'],risco[\"Risco1\"], test_size=0.2,random_state=45)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "clfL = LinearSVC().fit(X_train_tfidf, y_train)\n",
    "\n",
    "dfpna1.drop(['frases','riscos'], axis = 1, inplace=True)\n",
    "\n",
    "erros=[]\n",
    "for i in tqdm_notebook(range(len(dfpna1['txDescricao']))):\n",
    "    try:\n",
    "        dfpna1['Risco'][i]=clfL.predict(count_vect.transform([dfpna1['txDescricao'][i]]))\n",
    "    except Exception:\n",
    "        erros.append([df['ETPL_TX_DESCRICAO'],i])\n",
    "        continue \n",
    "\n",
    "partes_corpo=list(['mão','pé','cabeça','olhos','dedo','dedos','braço','braços','perna','pernas','boca','costela', 'orelha','joelho','joelhos','cabelo','cabelos','testa','pele','coluna'])\n",
    "\n",
    "implicação=list(['primeiros socorros','primeiro socorro','tratamento médico','restrição de atividades','restrição de atividade', 'incapacidade temporária total','incapacidade temporária parcial','incapacidade permanente total','dano pessoal','incapacidade permanente parcial','morte'])\n",
    "\n",
    "natureza_lesao=list(['contusão','esmagamento','lesão imediata', 'corte','laceração','ferida','punctura','queimadura','resvaladura', 'escaldadura','distensão','torção','luxacao','escoriaçao','abrasão','corte','outras lesões','fratura'])\n",
    "\n",
    "agente_causador= list(['queda','automovel','acessórios','ácido','acoplamento','acumuladores','aeronave','agente','agitador','água','alavanca','alcalis','álcool','alicate','alimentícios','amônia','andaime','animal','aparafusar','aparas','aquecedor','aquecimento','ar','arame','arco','areia','armário','armazenamento','arquivo','asfalto','balcão','bancada','bancos','barramento','barrica','barril','batedeira','bateria','benzeno','bicicleta','bomba','britador','broqueadeira','bruto','cabo','cabra','caçambas','cadeiras','cais','caixa','caixao','caixote','calandra','calcada','calcado','caldeira','calor','camara','caminhao','caminhonete','canal','canaleta','capacho','carbônico','carbono','carga','carreta','carro','carvao','cavadeira','ceramica','chama','chao','chapa','chave','cilindro','cilindros','cinzel','cloro','combustivel','composto','compostos','compressor','comprimido','construcao','conversor','copiadora','coque','corda','correias','corrente','cortar','degraus','descarga','descompressao','desmontavel','detergente','diesel','disjuntor','projecao','divisória','doca','edificações','edifício ','eletrica','eletrico','elevada','elevado','elevador','embalar','embarcacao','embreagem','emissor','empacotar','empilhadeira','energia','engradado','engrenagem','entulho','enxada','epi','equipamento','escada','escafandro','escavacao','escavacoes','escritorio','esferas','esmeril','esmerilhadeira','estante','estrada','estradas','estria','estrutura','estruturas','explosivo','extensivel','faca','facao','ferramenta','ferramentas','fibra','fichario','fogao','forca','forcado','forjar','forno','forracao','fosso','fossos','fragmentos','frasco','friccao','fundido','fundir','furadeira','fusivel','galeria','garfo','ancinho','garrafa','gas','gasolina','gerador','globo','gravidade','graxa','grifo','guilhotina','guincho','guindar','guindaste','halogenados','hidraulico','iluminacao','impacto','impressora','incandescente','infeccioso','inglesa','interruptor','ionizante','isolantes','janela','jato','lâmina','laminadora','lampada','lima','limpeza','liquefeito','liquido','liquidos','lixadeira','lubrificante','luminaria','macaco','macarico','machadinha','machado','madeira','malho','mangueira','manilhas','maquina','maritima','marreta','martelete','martelo','materiais','material','mecanica','mecanico','medicamentos','mergulho','mesa','metal','metalico','metalicos','mina','mineracao','minerais','misturador','mobiliario','moinho','monoxido','motocicleta','motoneta','motor','motriz','nafta','nitrogenio','oleo','orgaos','pa','painel','papel','parafina','parafuso','parasitario','particulas','passarela','pau','pedra','peneira','perfuracao','perfuratriz','petroleo','picareta','pier','piso','placa','plaina','plasticos','plataforma','pneumatico','poco','polia','politriz','ponte','ponteiro','porca','porta','portatil','portico','poste','pranchao','prego','prensa','pressao','processo','produto','produtos','puncao','quente','querosene','quimica','quina','radiacao','radioisotopo','raio','rebitadeira','rebite','recipientes','refratarios','reostato','resfriadores','resfriamento','retificador','ripa','rolante','roldana','rua','ruido','sabao','separador','serra','serrote','sino','socador','soldagem','soldar','solventes','sonda','substância','substâncias','sucata','sulfídrico','sustentação','tábua','talha','talhadeira','tambor','tanque','tapete','telhado','telhas','tenaz','terraplenagem','tesoura','tesourão','texteis','tijolos','tolueno','torques','torre','torres','tração','tranca','transformador','transmissão','transportador','trator','trilho','trilhos','trocadores','tubo','tubos','tubulação','tupia','turbina','válvula','vapor','vaso','vasos','vazador','veículo','vergalhão', 'verruma','viaduto','vidraria','vidro','xileno','pé de cabra'])\n",
    "\n",
    "dfpna1['riscos']=0\n",
    "dfpna1['frases']=0\n",
    "\n",
    "z#=dfpna1[\"Risco\"][0].tolist()\n",
    "z[0].split(\" - \")\n",
    "\n",
    "lista=[]\n",
    "for j in range(len(dfpna1['Risco'])):\n",
    "    temp= dfpna1['Risco'][j].tolist()\n",
    "    temp = temp[0].split (\" - \")\n",
    "    dfpna1['riscos'][j]=temp[0]\n",
    "    if (temp[1] == 'Não existe caracteristica de acidente relacionada a essa etapa'):\n",
    "        lista.append('Não existe caracteristica de acidente relacionada a essa etapa')\n",
    "    else:\n",
    "        y=temp[1].split()\n",
    "        lista.append(frases_composicao(y))\n",
    "\n",
    "#######################################fim###############################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Começo de criação de dicionário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate CountVectorizer()\n",
    "cv=CountVectorizer()\n",
    " \n",
    "# this steps generates word counts for the words in your docs\n",
    "word_count_vector=cv.fit_transform(dfalertas['conteudo'])\n",
    "word_count_vector.shape\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "# # print idf values\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n",
    " \n",
    "# # sort ascending\n",
    "# pd.set_option(\"display.max_rows\", None)\n",
    "# df_idf.sort_values(by=['idf_weights'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
