{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install pytest\n",
    "!pip install unittest\n",
    "!pip install tika\n",
    "!pip install PyPDF2\n",
    "!pip install python-docx\n",
    "!pip install -U spacy\n",
    "!python -m spacy download pt_core_news_sm\n",
    "!python -m spacy download pt_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import requests\n",
    "import PyPDF2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import pytest\n",
    "import unittest\n",
    "import spacy #NLP e Machine learning\n",
    "parser = English()\n",
    "from PyPDF2 import PdfFileWriter, PdfFileReader\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn import utils\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from string import punctuation\n",
    "from tika import parser\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_arquivos(endereco):\n",
    "    arquivos_pdf = = [arq for arq in os.listdir('endereco') if arq.lower().endswith(\".pdf\")]\n",
    "    arquivos_doc = [arq for arq in os.listdir('endereco') if arq.lower().endswith(\".docx\")]\n",
    "    return(arquivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp= parser.from_file(loc)['content']\n",
    "temp = temp.lower()\n",
    "temp = temp.replace(\"\\n\",\" \")\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('portuguese') + list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos_alertas=[]\n",
    "nomes=[]\n",
    "erros=[]\n",
    "for i in tqdm_notebook(range(len(arquivos))):\n",
    "    try:\n",
    "        temp= parser.from_file(arquivos[i])['content']\n",
    "        temp = temp.lower()\n",
    "        temp = temp.replace(\"\\n\",\" \")\n",
    "        nomes.append(arquivos[i])\n",
    "        textos_alertas.append(temp)\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            temp = open(arquivos[i], 'rb')#insere no python\n",
    "            pdf = PyPDF2.PdfFileReader(temp)#cria objeto \n",
    "            for j in range (pdf.getNumPages()):\n",
    "                textos = pdf.getPage(j).extractText()#retira o texto\n",
    "                textos = textos.lower()\n",
    "                textos = textos.replace(\"\\n\",\" \")\n",
    "                textos = textos.replace(\"   \",\" \")\n",
    "                nomes.append(arquivos[i])\n",
    "                textos_alertas.append(textos)\n",
    "        except Exception as e:\n",
    "              erros.append([e,i])\n",
    "        continue       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "document #= Document('loc.docx')\n",
    "for para in document.paragraphs:\n",
    "    print(para.text)\n",
    "for para in doc.paragraphs:\n",
    "    fullText.append(para.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load (\"pt_core_news_sm\")#(\"xx_ent_wiki_sm\")\n",
    "for i in range(len(textos_alertas)):\n",
    "    for j in range(len(textos_alertas)):\n",
    "        print(nlp(textos_alertas[i]).similarity(nlp(textos_alertas[j])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    \"\"\"\n",
    "    Limpar os textos, eliminando pontuações e converter \n",
    "    todo o texto com letras minusculas.\n",
    "    \"\"\"\n",
    "    pattern = \"[{}]\".format(string.punctuation)\n",
    "    text = [str(word).lower() for word in text]\n",
    "    text = [[re.sub(pattern, \"\", word) for word in str(words).split()] for words in text]\n",
    "    text = [[word for word in words if len(word)>1] for words in text]    \n",
    "    text = [' '.join(words) for words in text]\n",
    "    return np.array(textos_alertas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_all(text):\n",
    "    \"\"\"\n",
    "    Armazena em um vetor todas as palavras dos textos sem repetições.\n",
    "    \"\"\"\n",
    "    text_set = set()\n",
    "    for w in [words.split() for words in text]:\n",
    "        text_set.update(w)\n",
    "    return np.array(list(text_set))\n",
    "\n",
    "vocabulario = text_all(textos_alertas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform(text, words=vocabulario):\n",
    "    \"\"\"\n",
    "    Converte o texto em um vetor, onde compara se cada palavra obtida no vetor de \n",
    "    todas as palavras contém ou não em cada texto. \n",
    "    Insere 1 se sim e 0 se não.\n",
    "    \"\"\"\n",
    "    #return [1 if word in text.split() else 0 for word in words]\n",
    "    return [int(word in text.split()) for word in words]\n",
    "\n",
    "features = np.array(list(map(fit_transform,textos_alertas)))\n",
    "\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def cosine_similarity(v, w):\n",
    "    return np.dot(v, w)/np.sqrt(np.dot(v, v)*np.dot(w, w))    \n",
    "    #return np.dot(v, w)/(np.linalg.norm(v)*np.linalg.norm(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_simillarities(id_text, features=features, text=textos_alertas, n_text=len(textos_alertas)):\n",
    "    \"\"\"\n",
    "    Dado o texto a ser analisado, a função retorna em ordem descrecente quais os demais textos são\n",
    "    similares ao analisado. A função retorna matriz de 2 por n_text, onde a primeira e a segunda coluna\n",
    "    refere-se ao texto analisado e a similaridade do texto analisado, respectivamente.\n",
    "    \"\"\"\n",
    "    simillarity = [[cosine_similarity(features[id_text], feature), int(i)] for i, feature in enumerate(features)]\n",
    "    simillarity = np.array(sorted(simillarity, key=lambda sim: sim[0], reverse=True))    \n",
    "    return [[text[y], simillarity[x, 0]] for x, y in enumerate(np.int0(simillarity[1:,1]), 1)][:n_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(textos_alertas)):\n",
    "    for x, s in text_simillarities(0):\n",
    "        print(x, format(round(s, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "from unicodedata import normalize\n",
    "from nltk import ngrams\n",
    "\n",
    "#Regex para encontrar tokens\n",
    "REGEX_WORD = re.compile(r'\\w+')\n",
    "#Numero de tokens em sequencia\n",
    "N_GRAM_TOKEN = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faz a normalizacao do texto removendo espacos a mais e tirando acentos\n",
    "def text_normalizer(src):\n",
    "    return re.sub('\\s+', ' ',\n",
    "                normalize('NFKD', src)\n",
    "                   .encode('ASCII','ignore')\n",
    "                   .decode('ASCII')).lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faz o calculo de similaridade baseada no coseno\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        coef = float(numerator) / denominator\n",
    "        if coef > 1:\n",
    "            coef = 1\n",
    "        return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monta o vetor de frequencia da sentenca\n",
    "def sentence_to_vector(text, use_text_bigram):\n",
    "    words = REGEX_WORD.findall(text)\n",
    "    accumulator = []\n",
    "    for n in range(1,N_GRAM_TOKEN):\n",
    "        gramas = ngrams(words, n)\n",
    "        for grama in gramas:\n",
    "            accumulator.append(str(grama))\n",
    "\n",
    "    if use_text_bigram:\n",
    "        pairs = get_text_bigrams(text)\n",
    "        for pair in pairs:\n",
    "            accumulator.append(pair)\n",
    "\n",
    "    return Counter(accumulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtem a similaridade entre duas sentencas\n",
    "def get_sentence_similarity(sentence1, sentence2, use_text_bigram=False):\n",
    "    vector1 = sentence_to_vector(text_normalizer(sentence1), use_text_bigram)\n",
    "    vector2 = sentence_to_vector(text_normalizer(sentence2), use_text_bigram)\n",
    "    return cosine_similarity(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metodo de gerar bigramas de uma string\n",
    "def get_text_bigrams(src):\n",
    "    s = src.lower()\n",
    "    return [s[i:i+2] for i in range(len(s) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "similaridades=[]\n",
    "for i in tqdm_notebook(range(len(z))):   \n",
    "    if __name__ == \"__main__\":\n",
    "        w1 = z[i]\n",
    "        words = textos \n",
    "\n",
    "        #print('Busca: ' + w1)\n",
    "\n",
    "        #Nivel de aceite (40%)\n",
    "        #cutoff = 0.40\n",
    "        #Sentenças similares\n",
    "        \n",
    "        for w2 in words:\n",
    "            #print('\\nCosine Sentence --- ' + w2)\n",
    "\n",
    "            #Calculo usando similaridade do coseno com apenas tokens\n",
    "            similarity_sentence = get_sentence_similarity(w1, w2)\n",
    "            #print('\\tSimilarity sentence: ' + str(similarity_sentence))\n",
    "\n",
    "            #Calculo usando similaridade do coseno com tokens e com ngramas do texto\n",
    "            #similarity_sentence_text_bigram = get_sentence_similarity(w1, w2, use_text_bigram=True)\n",
    "            #print('\\tSimilarity sentence: ' + str(similarity_sentence_text_bigram))\n",
    "            similaridades.append((w1,w2,str(similarity_sentence)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similaridades"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
