{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vanessa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vanessa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import requests\n",
    "import PyPDF2\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import uuid, json\n",
    "import click\n",
    "import string\n",
    "import nltk\n",
    "import gensim\n",
    "#from textpack import tp\n",
    "import seaborn as sns\n",
    "import spacy #NLP e Machine learning\n",
    "#from spacy.lang import pt_core_news_sm\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "from io import StringIO\n",
    "from PyPDF2 import PdfFileWriter, PdfFileReader\n",
    "from tqdm import tqdm_notebook\n",
    "#tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from string import punctuation\n",
    "from tika import parser\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Edital.pdf'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arquivos = [arq for arq in os.listdir('./pdf_test') if arq.lower().endswith(\".pdf\")]\n",
    "arquivos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Vanessa\\\\Downloads\\\\scripts projeto de programação\\\\pdf_testEdital.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-1ce6f648a9c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtemp\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\Vanessa\\\\Downloads\\\\scripts projeto de programação\\\\pdf_test'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0marquivos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnomes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marquivos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtextos_alertas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tika\\parser.py\u001b[0m in \u001b[0;36mfrom_file\u001b[1;34m(filename, serverEndpoint, service, xmlContent, headers, config_path, requestOptions)\u001b[0m\n\u001b[0;32m     38\u001b[0m     '''\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mxmlContent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mservice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserverEndpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequestOptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequestOptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         output = parse1(service, filename, serverEndpoint, services={'meta': '/meta', 'text': '/tika', 'all': '/rmeta/xml'},\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tika\\tika.py\u001b[0m in \u001b[0;36mparse1\u001b[1;34m(option, urlOrPath, serverEndpoint, verbose, tikaServerJar, responseMimeType, services, rawResponse, headers, config_path, requestOptions)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mservice\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'/tika'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mresponseMimeType\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'text/plain'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'Accept'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mresponseMimeType\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Content-Disposition'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmake_content_disposition_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0municode_string\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0murlOrPath\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m_is_file_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlOrPath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m         status, response = callServer('put', serverEndpoint, service, f,\n\u001b[0;32m    337\u001b[0m                                       \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtikaServerJar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Vanessa\\\\Downloads\\\\scripts projeto de programação\\\\pdf_testEdital.pdf'"
     ]
    }
   ],
   "source": [
    "temp= parser.from_file('C:\\\\Users\\\\Vanessa\\\\Downloads\\\\scripts projeto de programação\\\\pdf_test'+ arquivos[1])['content']\n",
    "temp = temp.lower()\n",
    "temp = temp.replace(\"\\n\",\" \")\n",
    "nomes.append(arquivos[i])\n",
    "textos_alertas.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mais_palavras=['alerta', 'regra', 'ouro', 'ocorrencia', 'utilizados', 'palavras', 'apenas', 'acidentes','nº','preliminar','SMS', \n",
    "              'cuidados', 'sugeridos', 'aplicada', 'foto', 'fotos', 'data', 'emissão', 'atividade', 'presença', 'sms',\n",
    "              'devemos', 'evitar', 'palavra', 'alertas', 'possível', 'possíveis', '/' ,'cob./', 'mod./', 'mm./','`'] \n",
    "numeros=['0','1','2','3','4','5','6','7','8','9','2015','2016','2017','2018','01','02','03','04','05','06','07','08','09','10','11','12']\n",
    "stop_words = stopwords.words('portuguese') + list(punctuation) + mais_palavras + stopwords.words('english')+ numeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c2a9cb3c9d44d595a640835f73c780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "textos_alertas=[]\n",
    "nomes=[]\n",
    "erros=[]\n",
    "for i in tqdm_notebook(range(len(arquivos))):\n",
    "    try:\n",
    "        temp= parser.from_file('C:\\\\Users\\\\Vanessa\\\\Downloads\\\\scripts projeto de programação\\\\pdf_test'+ arquivos[i])['content']\n",
    "        temp = temp.lower()\n",
    "        temp = temp.replace(\"\\n\",\" \")\n",
    "        nomes.append(arquivos[i])\n",
    "        textos_alertas.append(temp)\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            temp = open('C:\\\\Users\\\\Vanessa\\\\Downloads\\\\scripts projeto de programação\\\\pdf_test'+ arquivos[i], 'rb')#insere no python\n",
    "            pdf = PyPDF2.PdfFileReader(temp)#cria objeto \n",
    "            for j in range (pdf.getNumPages()):\n",
    "                textos = pdf.getPage(j).extractText()#retira o texto\n",
    "                textos = textos.lower()\n",
    "                textos = textos.replace(\"\\n\",\" \")\n",
    "                textos = textos.replace(\"   \",\" \")\n",
    "                nomes.append(arquivos[i])\n",
    "                textos_alertas.append(textos)\n",
    "        except Exception as e:\n",
    "              erros.append([e,i])\n",
    "        continue       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[FileNotFoundError(2, 'No such file or directory'), 0],\n",
       " [FileNotFoundError(2, 'No such file or directory'), 1],\n",
       " [FileNotFoundError(2, 'No such file or directory'), 2]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load (\"pt_core_news_sm\")#(\"xx_ent_wiki_sm\")\n",
    "cont=0\n",
    "cont2=0\n",
    "with open('similaridadefile2.csv','w+') as similaridade2_file:\n",
    "    similaridade2_writer = csv.writer(similaridade2_file, delimiter=';', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    for sms in tqdm_notebook(textos):\n",
    "        sms_i = nlp(sms)\n",
    "        for n in c:\n",
    "            similaridade2_writer.writerow([nome_arq[cont],cd_pt[cont2],cd_plat[cont2], nlp(n), sms_i.similarity(nlp(n))]) \n",
    "            cont2+=1\n",
    "        cont+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    \"\"\"\n",
    "    Limpar os textos, eliminando pontuações e converter \n",
    "    todo o texto com letras minusculas.\n",
    "    \"\"\"\n",
    "    pattern = \"[{}]\".format(string.punctuation)\n",
    "    text = [str(word).lower() for word in text]\n",
    "    text = [[re.sub(pattern, \"\", word) for word in str(words).split()] for words in text]\n",
    "    text = [[word for word in words if len(word)>1] for words in text]    \n",
    "    text = [' '.join(words) for words in text]\n",
    "    return np.array(textos_alertas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_all(text):\n",
    "    \"\"\"\n",
    "    Armazena em um vetor todas as palavras dos textos sem repetições.\n",
    "    \"\"\"\n",
    "    text_set = set()\n",
    "    for w in [words.split() for words in text]:\n",
    "        text_set.update(w)\n",
    "    return np.array(list(text_set))\n",
    "\n",
    "\n",
    "vocabulario = text_all(z1['texto_alerta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_transform(text, words=vocabulario):\n",
    "    \"\"\"\n",
    "    Converte o texto em um vetor, onde compara se cada palavra obtida no vetor de \n",
    "    todas as palavras contém ou não em cada texto. \n",
    "    Insere 1 se sim e 0 se não.\n",
    "    \"\"\"\n",
    "    #return [1 if word in text.split() else 0 for word in words]\n",
    "    return [int(word in text.split()) for word in words]\n",
    "\n",
    "features = np.array(list(map(fit_transform,z1['texto_alerta'])))\n",
    "\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def cosine_similarity(v, w):\n",
    "    return np.dot(v, w)/np.sqrt(np.dot(v, v)*np.dot(w, w))    \n",
    "    #return np.dot(v, w)/(np.linalg.norm(v)*np.linalg.norm(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_simillarities(id_text, features=features, text=z1['texto_alerta'],title=z1['nome_alerta'], n_text=10):\n",
    "    \"\"\"\n",
    "    Dado o texto a ser analisado, a função retorna em ordem descrecente quais os demais textos são\n",
    "    similares ao analisado. A função retorna matriz de 2 por n_text, onde a primeira e a segunda coluna\n",
    "    refere-se ao texto analisado e a similaridade do texto analisado, respectivamente.\n",
    "    \"\"\"\n",
    "    simillarity = [[cosine_similarity(features[id_text], feature), int(i)] for i, feature in enumerate(features)]\n",
    "    simillarity = np.array(sorted(simillarity, key=lambda sim: sim[0], reverse=True))    \n",
    "    return [[title[y], text[y], simillarity[x, 0]] for x, y in enumerate(np.int0(simillarity[1:,1]), 1)][:n_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while i<= 500000:\n",
    "    data=[]\n",
    "    for etapa in tqdm_notebook(range(50000)):\n",
    "        for x, t, s in text_simillarities(etapa):\n",
    "            data.append([z1['ETPL_CD_ETAPA'][i], z1['PLAT_CD_PLATAF'][i], z1['UNID_NR_UNIDADE_EXEC'][i],\n",
    "                         z1['SERV_CD_SERVICO'][i], z1['ETPL_TX_DESCRICAO'][i], x, t, format(round(s, 8))])\n",
    "        #r = requests.post(url = API_ENDPOINT, data = data) \n",
    "        i+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import re\n",
    "import math\n",
    "\n",
    "from collections import Counter\n",
    "from unicodedata import normalize\n",
    "from nltk import ngrams\n",
    "\n",
    "#Regex para encontrar tokens\n",
    "REGEX_WORD = re.compile(r'\\w+')\n",
    "#Numero de tokens em sequencia\n",
    "N_GRAM_TOKEN = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faz a normalizacao do texto removendo espacos a mais e tirando acentos\n",
    "def text_normalizer(src):\n",
    "    return re.sub('\\s+', ' ',\n",
    "                normalize('NFKD', src)\n",
    "                   .encode('ASCII','ignore')\n",
    "                   .decode('ASCII')).lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Faz o calculo de similaridade baseada no coseno\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        coef = float(numerator) / denominator\n",
    "        if coef > 1:\n",
    "            coef = 1\n",
    "        return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monta o vetor de frequencia da sentenca\n",
    "def sentence_to_vector(text, use_text_bigram):\n",
    "    words = REGEX_WORD.findall(text)\n",
    "    accumulator = []\n",
    "    for n in range(1,N_GRAM_TOKEN):\n",
    "        gramas = ngrams(words, n)\n",
    "        for grama in gramas:\n",
    "            accumulator.append(str(grama))\n",
    "\n",
    "    if use_text_bigram:\n",
    "        pairs = get_text_bigrams(text)\n",
    "        for pair in pairs:\n",
    "            accumulator.append(pair)\n",
    "\n",
    "    return Counter(accumulator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtem a similaridade entre duas sentencas\n",
    "def get_sentence_similarity(sentence1, sentence2, use_text_bigram=False):\n",
    "    vector1 = sentence_to_vector(text_normalizer(sentence1), use_text_bigram)\n",
    "    vector2 = sentence_to_vector(text_normalizer(sentence2), use_text_bigram)\n",
    "    return cosine_similarity(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metodo de gerar bigramas de uma string\n",
    "def get_text_bigrams(src):\n",
    "    s = src.lower()\n",
    "    return [s[i:i+2] for i in range(len(s) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "similaridades=[]\n",
    "for i in tqdm_notebook(range(10000)):   \n",
    "    if __name__ == \"__main__\":\n",
    "        w1 = dfetap['texto_concat'][i]\n",
    "        words = textos \n",
    "\n",
    "        #print('Busca: ' + w1)\n",
    "\n",
    "        #Nivel de aceite (40%)\n",
    "        #cutoff = 0.40\n",
    "        #Sentenças similares\n",
    "        \n",
    "        for w2 in words:\n",
    "            #print('\\nCosine Sentence --- ' + w2)\n",
    "\n",
    "            #Calculo usando similaridade do coseno com apenas tokens\n",
    "            similarity_sentence = get_sentence_similarity(w1, w2)\n",
    "            #print('\\tSimilarity sentence: ' + str(similarity_sentence))\n",
    "\n",
    "            #Calculo usando similaridade do coseno com tokens e com ngramas do texto\n",
    "            #similarity_sentence_text_bigram = get_sentence_similarity(w1, w2, use_text_bigram=True)\n",
    "            #print('\\tSimilarity sentence: ' + str(similarity_sentence_text_bigram))\n",
    "            similaridades.append((w1,w2,str(similarity_sentence)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
